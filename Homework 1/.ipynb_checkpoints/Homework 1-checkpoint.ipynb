{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Task-1.-Compiling-Ebola-Data\"><span class=\"toc-item-num\">Task 1.&nbsp;&nbsp;</span>Compiling Ebola Data</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-2.-RNA-Sequences\"><span class=\"toc-item-num\">Task 2.&nbsp;&nbsp;</span>RNA Sequences</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-3.-Class-War-in-Titanic\"><span class=\"toc-item-num\">Task 3.&nbsp;&nbsp;</span>Class War in Titanic</a></div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'Data' # Use the data folder provided in Tutorial 02 - Intro to Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Compiling Ebola Data\n",
    "\n",
    "The `DATA_FOLDER/ebola` folder contains summarized reports of Ebola cases from three countries (Guinea, Liberia and Sierra Leone) during the recent outbreak of the disease in West Africa. For each country, there are daily reports that contain various information about the outbreak in several cities in each country.\n",
    "\n",
    "Use pandas to import these data files into a single `Dataframe`.\n",
    "Using this `DataFrame`, calculate for *each country*, the *daily average per month* of *new cases* and *deaths*.\n",
    "Make sure you handle all the different expressions for *new cases* and *deaths* that are used in the reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2>Solution</h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<p>\n",
    "First we will create few functions that will be useful later. \n",
    "</p>\n",
    "\n",
    "<br>\n",
    "<b>fetch_forward_cols</b> is function for reordering the columns. First argument is DataFrame object whose columns we want to reorder. Second argument is list of columns that we want to be shown first in defined order. The rest of the columns will appear after these columns and will not be specificaly ordered.\n",
    "<br>\n",
    "<br>\n",
    "<b>load_files </b> will load the data for specific country (for all dates for which the data exists). Additionaly, as input argument we specify which column should be used as date column. Return value is DataFrame sorted by date, with column names in lower case. \n",
    "<br>\n",
    "<br>\n",
    "<b>fix_columns </b> is needed because column names are not consistent for each country. This function provides posibility to rename columns, and also to extract columns of interest to be shown first.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<b>load_guinea, load_sl, load_liberia </b> are functions for loading the data for each country.  \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<p>\n",
    "After loading the data, we merge it to one DataFrame. There are many columns, as each country has different city names. Because of this and the fact that we are not interested in statistics of each city, we will bound the DataFrame to fewer number of columns.\n",
    "</p>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<b>We notice few interesting things about our data. </b>\n",
    "\n",
    "<p>\n",
    "<br>\n",
    "For Guinea we notice that there are two cities named similar - Nzerekore and Mzerekore. After searching further, we find that there is city named Nzerekore in this country, but not named Mzerekore. So we can conclude that another city name is typing mistake (n and m are close on the keyboard, so this might be the reason). As we have 'Totals' column which presents sum for all cities, we won't try to correct this mistake. But it is interesting to notice it. \n",
    "<br>\n",
    "<br>\n",
    "Apart from this, 'totals' column should present the sum for specific description, for all cities. If we check this, we get that in some cases this is not really true. If we take Guinea for example, there are few cases when 'totals' column is showing less number then the one we get by summing cities' columns. We will take 'Totals' column as the real and correct one.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Also we can notice that number od new cases is not consistent with totals column. Sometimes it happens that new cases in a day difference shows one value, and when we compare total cases from start of the measurement we get another vaulue. Because of this, we will calculate mean values for both cases. \n",
    "</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-572915c01d56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# load the data for each country\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mdf_guinea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_guinea\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mdf_sl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_sl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mdf_liberia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_liberia\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-572915c01d56>\u001b[0m in \u001b[0;36mload_guinea\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_guinea\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"guinea_data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"guinea\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-572915c01d56>\u001b[0m in \u001b[0;36mload_files\u001b[0;34m(country_dir, country_name, date_col)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Write your answer here\n",
    "\n",
    "from glob import glob\n",
    "import os.path as path\n",
    "\n",
    "data_root = path.join(\"Data\", \"ebola\")\n",
    "\n",
    "def fetch_forward_cols(df, what):\n",
    "    other_cols = df.columns.difference(what).values.tolist()\n",
    "    reordered_cols = what + other_cols\n",
    "    return df[reordered_cols]\n",
    "\n",
    "\n",
    "def load_files(country_dir, country_name, date_col):\n",
    "    files = glob(path.join(data_root, country_dir, \"*.csv\"))\n",
    "    \n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        dfs.append(pd.read_csv(file, parse_dates=[date_col]))\n",
    "        \n",
    "    df = pd.concat(dfs)\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df['country'] = country_name\n",
    "    df = df.sort_values(by='date')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fix_columns(df, rename=None):\n",
    "    if rename is not None:\n",
    "        df = df.rename(columns=rename)\n",
    "        \n",
    "    df = fetch_forward_cols(df, ['date', 'country', 'description', 'totals'])\n",
    "    df['description'] = df['description'].str.lower()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_guinea():\n",
    "    df = load_files(\"guinea_data\", \"guinea\", \"Date\")\n",
    "    df = fix_columns(df)\n",
    "    return df\n",
    "    \n",
    "def load_sl():\n",
    "    df = load_files(\"sl_data\", \"sierra leone\", \"date\")\n",
    "    df = fix_columns(df, rename={'variable': 'description', 'national': 'totals'})\n",
    "    return df\n",
    "\n",
    "def load_liberia():\n",
    "    df = load_files(\"liberia_data\", \"liberia\", \"Date\")\n",
    "    df = fix_columns(df, rename={'variable': 'description', 'national': 'totals'})\n",
    "    return df\n",
    "\n",
    "# load the data for each country\n",
    "df_guinea = load_guinea()\n",
    "df_sl = load_sl()\n",
    "df_liberia = load_liberia()\n",
    "\n",
    "# merge all data into one DataFrame\n",
    "df = pd.concat([df_guinea, df_sl, df_liberia])\n",
    "\n",
    "# we are choosing specific columns of interest\n",
    "df = df[['date', 'country', 'description', 'totals']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<br>\n",
    "<p>\n",
    "<b>sanity_check </b> function is used to check if for each country we have the same number of unique dates and specific description columns. This functions is used later when we introduce regular expressions for handling the problem of columns names in different files, for different countries. \n",
    "<br>\n",
    "\n",
    "<br>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sanity_check(df, country, target_col, verbose=False):\n",
    "    if target_col is None:\n",
    "        return True\n",
    "    \n",
    "    tmp_df = df[df.country == country]\n",
    "    total_dates = len(tmp_df.date.unique())\n",
    "    target_col_cnt = tmp_df[tmp_df.description.str.contains(target_col)].description.count()\n",
    "    \n",
    "    if total_dates != target_col_cnt or total_dates == 0:\n",
    "        print(country + \" - \" + target_col + \": FAILED !!!\")\n",
    "        print(total_dates, target_col_cnt)\n",
    "        return False\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(country + \" - \" + target_col + \": passed\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<br>\n",
    "<p>\n",
    "\n",
    "In the code below we are introducing the list of regular expressions for descriptions. This should help us to solve and handle the problem of different descriptions for different countries and files, which corespond to the same statistics. We want to come to the point where these descriptions are named same for each country. Because of this, we map country specific descriptions to common ones that will be used later.\n",
    "\n",
    "</p>\n",
    "<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex_db_raw = [\n",
    "    ('guinea', 'new_cases_sum', 'total new cases registered so far'), # new cases\n",
    "    ('guinea', 'new_cases_confirmed', 'new cases of confirmed$'),\n",
    "    ('guinea', 'new_cases_probable', 'new cases of probable'),\n",
    "    ('guinea', 'new_cases_suspected', 'new cases of suspect'),\n",
    "    \n",
    "    ('guinea', 'total_cases_sum', 'cumulative'),                     # total cases\n",
    "    ('guinea', 'total_cases_confirmed', 'total cases of confirmed'),\n",
    "    ('guinea', 'total_cases_probable', 'total cases of probable'),\n",
    "    ('guinea', 'total_cases_suspected', 'total cases of suspect'),\n",
    "    \n",
    "    ('guinea', 'new_deaths_sum', 'new deaths registered$|new deaths registered today$'),  # new deaths\n",
    "    \n",
    "    \n",
    "    \n",
    "    ('liberia', 'new_cases_sum', None),                            # new cases\n",
    "    ('liberia', 'new_cases_confirmed', 'new case.*confirmed'),\n",
    "    ('liberia', 'new_cases_probable', 'new case.*probable'),\n",
    "    ('liberia', 'new_cases_suspected', 'new case.*suspected'),\n",
    "    \n",
    "    ('liberia', 'total_cases_sum', None),                          # total cases\n",
    "    ('liberia', 'total_cases_confirmed', 'total confirmed cases'),\n",
    "    ('liberia', 'total_cases_probable', 'total probable cases'),\n",
    "    ('liberia', 'total_cases_suspected', 'total suspected cases'),\n",
    "    \n",
    "    ('liberia', 'new_deaths_sum', 'newly reported deaths$'),\n",
    "    \n",
    "    \n",
    "    ('sierra leone', 'new_cases_sum', None),                       # new cases\n",
    "    ('sierra leone', 'new_cases_confirmed', 'new_confirmed'),\n",
    "    ('sierra leone', 'new_cases_probable', 'new_probable'),\n",
    "    ('sierra leone', 'new_cases_suspected', 'new_suspected'),\n",
    "    \n",
    "    ('sierra leone', 'total_cases_sum', None),                     # total cases\n",
    "    ('sierra leone', 'total_cases_confirmed', 'cum_confirmed'),\n",
    "    ('sierra leone', 'total_cases_probable', 'cum_probable'),\n",
    "    ('sierra leone', 'total_cases_suspected', 'cum_suspected'),\n",
    "    \n",
    "    ('sierra leone', 'new_deaths_sum', None),\n",
    "    ('sierra leone', 'new_deaths_confirmed', 'death_confirmed'),\n",
    "    ('sierra leone', 'new_deaths_probable', 'death_probable'),\n",
    "    ('sierra leone', 'new_deaths_suspected', 'death_suspected'),\n",
    "]\n",
    "\n",
    "for (country, _, target_col) in regex_db_raw:\n",
    "    sanity_check(df, country, target_col, verbose=False)\n",
    "\n",
    "regex_db = pd.DataFrame(regex_db_raw, columns=['country', 'common_name', 'selector'])\n",
    "regex_db.set_index(['country', 'common_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<p>\n",
    "<br>\n",
    "Above we created the list of regular expressions for the descriptions for each country. This is done just for the descriptions of interest, which will be used when calculating the statistics.Beside of that, we also used function sanity_check described earlier, to be sure that data is valid for each country.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Here we notice that Sierra Leone does not have sum column neither for new cases, nor for new deaths. So we will also need to take this fact into account. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Function <b>make_master_df</b> is used for creating dataframe with three columns -new_cases_sum, new_deaths_sum, total_cases_sum. For each country we get these value using the regex list of corresponding descriptions. We also handle the case when total_sum is None (as mentioned this is the case with Sierra Leone). We calculate total sum out of \n",
    "confirmed, probable and suspected cases.\n",
    "\n",
    "<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_master_df(df, regex_db):\n",
    "    elements  = {\n",
    "        'new_cases_sum'  : ['new_cases_confirmed'  , 'new_cases_probable'  , 'new_cases_suspected'   ],\n",
    "        'total_cases_sum': ['total_cases_confirmed', 'total_cases_probable', 'total_cases_suspected' ],\n",
    "        'new_deaths_sum' : ['new_deaths_confirmed' , 'new_deaths_probable' , 'new_deaths_suspected'  ],\n",
    "    }\n",
    "    \n",
    "    # return country specific description based on regex list\n",
    "    def get_selector(country, target):\n",
    "        return regex_db.loc[country, target]['selector']\n",
    "    \n",
    "    # return value where description contains country specific description\n",
    "    def get_selected(this_df, selector):\n",
    "        return this_df[this_df['description'].str.contains(selector)].copy()\n",
    "\n",
    "    new_df = pd.DataFrame(columns=['country', 'month', 'date', 'description', 'totals', ])\n",
    "    \n",
    "    # for each country calculate values\n",
    "    for country in df['country'].unique():\n",
    "        country_df = df[df['country'] == country]\n",
    "        \n",
    "        # target = new cases, new deaths, total cases\n",
    "        for target in list(elements.keys()):\n",
    "            \n",
    "            # get country specific description\n",
    "            selector = get_selector(country, target)\n",
    "            \n",
    "            # for this description (selector) get all values for current country\n",
    "            if selector is not None: \n",
    "                res = get_selected(country_df, selector)\n",
    "                res.description = target\n",
    "                res.totals = res.totals.astype(float)\n",
    "                \n",
    "                # append this to resulting dataframe\n",
    "                new_df = new_df.append(res)\n",
    "                \n",
    "            # selection is none in case of sierra leone\n",
    "            # so then we calculate as described in text above\n",
    "            elif selector is None:\n",
    "                partials = []\n",
    "                for elem in elements[target]:\n",
    "                    tmp_selector = get_selector(country, elem)\n",
    "                    partial = get_selected(country_df, tmp_selector)\n",
    "                    partials.append(partial)\n",
    "                    \n",
    "                res = pd.concat(partials)\n",
    "                res.sort_values(by='date', inplace=True)\n",
    "                \n",
    "                res['totals'] = res.totals.astype(str).str.replace(\",\", \"\")\n",
    "                res['totals'] = res.totals.astype(float)\n",
    "                \n",
    "                \n",
    "                target_sum = res.groupby('date')['totals'].sum()\n",
    "                target_sum = target_sum.to_frame()\n",
    "                target_sum['date'] = target_sum.index\n",
    "                target_sum['country'] = country\n",
    "                target_sum['description'] = target\n",
    "                target_sum['totals'] = target_sum['totals'].astype(float)\n",
    "                                \n",
    "                new_df = new_df.append(target_sum)\n",
    "                \n",
    "    new_df['month'] = new_df.date.dt.month\n",
    "    new_df = new_df.pivot_table(index=['country', 'month', 'date'], columns=['description'], values='totals')\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<br>\n",
    "<h3>Two ways of calculating statistics</h3>\n",
    "\n",
    "<br>\n",
    "\n",
    "<p>\n",
    "Function <b>get_standard_analysis</b> takes dataframe as argument, and calculates mean values for new cases and new deaths for it.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Function <b>get_totals_analysis</b> calculates additional statistics based on values for counter for number of cases since the measurment started. We wanted to see how is this statistics different from the other one - when we use total number of cases per day.\n",
    "<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_standard_analysis(df):\n",
    "    grouped = df.groupby(level=['country', 'month'])\n",
    "    means = grouped.agg({'new_cases_sum': 'mean', 'new_deaths_sum': 'mean'})\n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_totals_analysis(df):\n",
    "    local_df = df.copy()\n",
    "    local_df['day'] = df.index.get_level_values('date')\n",
    "    \n",
    "    grouped = local_df.groupby(level=['country', 'month'])\n",
    "    res = grouped.agg({'day': ['first', 'last'], 'total_cases_sum': ['first', 'last']})\n",
    "    \n",
    "    res['days_diff'] = (res['day']['last'] - res['day']['first']).dt.days\n",
    "    res['cases_diff'] = res['total_cases_sum','last'] - res['total_cases_sum', 'first']\n",
    "    res['cases_avg'] = res['cases_diff'] / res['days_diff']\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = make_master_df(df, regex_db)\n",
    "master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<h3>Final result</h3>\n",
    "\n",
    "<br>\n",
    "\n",
    "<p>\n",
    "Final results presented below shows average number of new cases, new deaths and total cases (using total counter which presents number of total cases for each country since the measurement started). \n",
    "\n",
    "We can notice that statistics is different for new cases if we use number of total cases per day, compared to when we use counter from begining of the measurements.\n",
    "<br>\n",
    "<br>\n",
    "Explanation for this is the way that we calculate the statistics. When using sum of total values per day, statistics depends of number of days, but also from the fact how far these days are from the begining of the measurements. The function of number of sick people with time is not linear function. Because of this we will not get the same result if we have first five days of the disease, or the five days after the disease has spreaded. From the same reason, mean value when using total counter depends of the day difference for each month. The bigger the difference is, the better the accuracy. So when we have total people for first day of the month and the last day of month, we can have good estimation of mean value of new cases. \n",
    "\n",
    "<br>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = get_standard_analysis(master_df)\n",
    "res2 = get_totals_analysis(master_df)\n",
    "\n",
    "final_results = pd.concat([res1, res2['cases_avg']], axis=1)\n",
    "final_results.fillna(0, inplace=True)\n",
    "final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. RNA Sequences\n",
    "\n",
    "In the `DATA_FOLDER/microbiome` subdirectory, there are 9 spreadsheets of microbiome data that was acquired from high-throughput RNA sequencing procedures, along with a 10<sup>th</sup> file that describes the content of each. \n",
    "\n",
    "Use pandas to import the first 9 spreadsheets into a single `DataFrame`.\n",
    "Then, add the metadata information from the 10<sup>th</sup> spreadsheet as columns in the combined `DataFrame`.\n",
    "Make sure that the final `DataFrame` has a unique index and all the `NaN` values have been replaced by the tag `unknown`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "So what we did first is to import the 9 spreadsheets in one single dataframe, and doing that, adding a column 'Origin' to know from which file the data come from. And we also load the data from the 10th spreadsheet.\n",
    "\n",
    "After that, add two more columns to the dataframe, 'GROUP' and 'SAMPLE', and for the values, we put in it what we have on the 'Origin' column. Now the idea is to replace the values that are in our two new columns by the values we have in the metadata file. In order to do that, we search in the metadata file which term is going to replace the existing term. So we first search the row that we need, and then we choose the column, so 'GROUP' or 'SAMPLE'.\n",
    "\n",
    "We now can drop the 'Origin' column as we don't need it anymore, and we can replace all the NaN value by the 'unknown' string.\n",
    "\n",
    "And after that, we do a unique index for the data by creating a new range going from 0 to the length of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# We want all the xls files that are in microbiome folder\n",
    "frame_micro = pd.DataFrame()\n",
    "list_ = []\n",
    "for i in range(1, 10):\n",
    "    i = str(i)\n",
    "    data = pd.read_excel(DATA_FOLDER + \"/microbiome/MID\" + i + \".xls\", sheetname = \"Sheet 1\", header = None)\n",
    "    data['Origin'] = \"MID\" + i # this way we will know after from which file the data come\n",
    "    list_.append(data)\n",
    "frame_micro = pd.concat(list_)\n",
    "\n",
    "# We read the 10th spreadsheet\n",
    "frame_metadata = pd.read_excel(DATA_FOLDER + \"/microbiome/metadata.xls\", sheetname = \"Sheet1\")\n",
    "\n",
    "# We make the two new columns we need for the dataframe and we copy the value that is in the column \"origin\",\n",
    "# this way, we only have to replace the value by what is on the metadata dataframe\n",
    "frame_micro['GROUP'] = frame_micro['Origin']\n",
    "frame_micro['SAMPLE'] = frame_micro['Origin']\n",
    "\n",
    "# Here we do several things at the same time. The idea is to replace in column 'GROUP' and 'SAMPLE' the value by the value that is given in \n",
    "# the metadata file. For example, we know that for row whose origin is MID1, the 'GROUP' column should be filled with \"EXTRACTION CONTROL\".\n",
    "# So the idea is to search in the metadata file which term is going to replace the exisiting term. For that we search the row that contains for example\n",
    "# MID1, and when we have the row, we just have to take the right column ('GROUP' or 'SAMPLE').\n",
    "for i in range(1, 10):\n",
    "    i = str(i)\n",
    "    frame_micro['GROUP'] = frame_micro['GROUP'].replace(['MID' + i], frame_metadata.loc[frame_metadata['BARCODE'] == 'MID' + i]['GROUP'])\n",
    "    frame_micro['SAMPLE'] = frame_micro['SAMPLE'].replace(['MID' + i], frame_metadata.loc[frame_metadata['BARCODE'] == 'MID' + i]['SAMPLE'])\n",
    "\n",
    "# We delete the 'Origin' column as we don't need it anymore\n",
    "frame_micro = frame_micro.drop('Origin', 1)    \n",
    "\n",
    "# We replace all the Nan values in the dataframe by 'unknown'\n",
    "frame_micro = frame_micro.fillna('unknown')\n",
    "\n",
    "# We make a unique index for the data\n",
    "ind = [i for i in range (len(frame_micro))]\n",
    "frame_micro.reset_index(drop=True)\n",
    "frame_micro.index = range(len(frame_micro.index))\n",
    "\n",
    "frame_micro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Class War in Titanic\n",
    "\n",
    "Use pandas to import the data file `Data/titanic.xls`. It contains data on all the passengers that travelled on the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(filename=DATA_FOLDER+'/titanic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following questions state clearly your assumptions and discuss your findings:\n",
    "1. Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`. \n",
    "2. Plot histograms for the *travel class*, *embarkation port*, *sex* and *age* attributes. For the latter one, use *discrete decade intervals*. \n",
    "3. Calculate the proportion of passengers by *cabin floor*. Present your results in a *pie chart*.\n",
    "4. For each *travel class*, calculate the proportion of the passengers that survived. Present your results in *pie charts*.\n",
    "5. Calculate the proportion of the passengers that survived by *travel class* and *sex*. Present your results in *a single histogram*.\n",
    "6. Create 2 equally populated *age categories* and calculate survival proportions by *age category*, *travel class* and *sex*. Present your results in a `DataFrame` with unique index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Solution</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1. Describe the type and the value range of each attribute. Indicate and transform the attributes that can be Categorical</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the raw data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the data\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_palette(\"Set2\", 10)\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "titanic_data = pd.read_excel(\"Data/titanic.xls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we look for the attributes that can be categorical.<br>\n",
    "\"Categoricals are a pandas data type, which corresponds to categorical variables\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look on the raw data to find potential categorical attributes\n",
    "for attribute in titanic_data.columns:\n",
    "    unique_values = titanic_data[attribute].unique()\n",
    "    print('{} {} values: '.format(attribute, titanic_data[attribute].dtype), end='')\n",
    "    if len(unique_values) <= 5:\n",
    "        print(unique_values)\n",
    "    else:\n",
    "        print('{} possible values'.format(len(unique_values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array presents our choices to decide which attributes are good candidates to become categorical.\n",
    "\n",
    "| Attribute | Description | Categorical |\n",
    "| :------ | :----------- | :----------- |\n",
    "| pclass   | Can be 1,2 or 3 | Yes |\n",
    "| survived | Can be 0 or 1 | Yes |\n",
    "| name   | String variable | No |\n",
    "| sex | Can be 'female' or 'male' | Yes |\n",
    "| age   | From 0 to 80 with NaN values to discard | No |\n",
    "| sibsp   | From 0 to 8 | No |\n",
    "| parch   | From 0 to 9 | No |\n",
    "| ticket   | String variable | No |\n",
    "| fare   | From 0 to 512.3292 | No |\n",
    "| cabin   | Just the letter is important as it is the floor | Yes |\n",
    "| embarked   | ['C', 'Q', 'S'] | Yes |\n",
    "| boat   | String variable | No |\n",
    "| body   | From 1 to 328 | No |\n",
    "| home.dest   | Character | No |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pclass becomes categorical\n",
    "titanic_data.pclass = titanic_data.pclass.astype('category')\n",
    "\n",
    "#survived becomes categorical\n",
    "titanic_data.survived = titanic_data.survived.astype('category')\n",
    "\n",
    "#sex becomes categorical\n",
    "titanic_data.sex = titanic_data.sex.astype('category')\n",
    "\n",
    "#cabin - floor becomes categorical : just the first letter is enough, as it represents the floor\n",
    "titanic_data['floor'] = titanic_data.cabin.str[:1]\n",
    "titanic_data['floor'].fillna('unknown',  inplace=True)\n",
    "titanic_data['floor'].astype('category')\n",
    "\n",
    "#embarked becomes categorical\n",
    "titanic_data.embarked = titanic_data.embarked.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot data functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below help us to plot nicely our results. We use the library seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_data_bar(data, ax, title='', ylabel=''):\n",
    "    ax.set_xlabel(data.name)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    sns.barplot(x=data.keys(), y=data.values,  ax=ax, palette=\"hls\")\n",
    "    locs, labels = plt.xticks()\n",
    "    plt.setp(labels, rotation=90)\n",
    "    \n",
    "\n",
    "def plot_data_pie(data, ax, labels, explode, title=''):\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.pie(data, labels=labels, autopct='%1.1f%%', explode=explode, startangle=140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 2. Plot histograms for the travel class, embarkation port, sex and age attributes. For the latter one, use discrete decade intervals. </b>\n",
    "\n",
    "Here, we plot the repartition of the passengers by travel class, embarkation port, sex and age. <br>\n",
    "NaN values are not taken into account because we assume they don't represent usefull data.<br>\n",
    "As you can see, the age data it splitted with the function <i>pd.cut()</i> to create decades groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 2, figsize=(16,16))\n",
    "\n",
    "# Histogram travel class\n",
    "data = titanic_data.pclass.value_counts(dropna=False)\n",
    "plot_data_bar(data, axes[0, 0], 'Passengers by travel class', 'passengers')\n",
    "\n",
    "# Embarkation port\n",
    "data = titanic_data.embarked.value_counts(dropna=False)\n",
    "plot_data_bar(data, axes[0, 1], 'Passengers by embarkation port', 'passengers')\n",
    "\n",
    "# Sex\n",
    "data = titanic_data.sex.value_counts(dropna=False)\n",
    "plot_data_bar(data, axes[1, 0], 'Passengers by sex', 'passengers')\n",
    "\n",
    "# Age\n",
    "age_groups = pd.cut(titanic_data.age, np.arange(0, titanic_data.age.max() + 10, 10)).value_counts()\n",
    "plot_data_bar(age_groups, axes[1, 1], 'Passengers by age', 'passengers')\n",
    "\n",
    "\n",
    "# two men are missing => isnull doesn't find this! => empty strings\n",
    "num_s = len(titanic_data.query('embarked == \\'S\\''))\n",
    "num_q = len(titanic_data.query('embarked == \\'Q\\''))\n",
    "num_c = len(titanic_data.query('embarked == \\'C\\''))\n",
    "print (\"Passengers with embarkation port:\", num_s+num_q+num_c)\n",
    "\n",
    "#Some passengers does not have age\n",
    "null_age = titanic_data['age'].isnull().sum()\n",
    "print(\"Passengers with no age:\", null_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 3. Calculate the proportion of passengers by cabin floor. Present your results in a pie chart.</h4>\n",
    "<br>\n",
    "<div>\n",
    "<p>\n",
    "\n",
    "There are some passingers that are listed in cabin names \"F E..\", \"F G..\". These are most probably input errors. We will just take these people as they are on both of the floors, since we don't know for sure. There is 7 people more than it really was on the ship, so we know there are 7 people more that we counted on two floors. <br>We are of course aware of the fact that these people are counted on both floors because there is same posibility they were in one or another cabin. Of course, we could try to search if there is another person who is in one of two possible cabins, and then check if this cabin was for one or two people. But since we don't have information of how many people there were in specific cabin, we will just count these people as they were in both cabins.  \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(1, 2, figsize=(16,8))\n",
    "\n",
    "#We use our floor column that is categorical now\n",
    "data = titanic_data.floor.value_counts()\n",
    "explode = (0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1)\n",
    "labels = data.index\n",
    "plot_data_pie(data.values, axes[0], labels, explode, 'Passengers by cabin floor - unknown floor included')\n",
    "\n",
    "data = data[1:] #without unknown\n",
    "explode = (0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1)\n",
    "labels = data.index\n",
    "plot_data_pie(data.values, axes[1], labels, explode, 'Passengers by cabin floor - only known floors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 4. For each travel class, calculate the proportion of the passengers that survived. Present your results in pie charts.</h4>\n",
    "<div>\n",
    "<p>\n",
    "We plot a pie chart for each travel class.\n",
    "We can see that in first class, there is a much higher chance of survival (~ 62%), whereas in the third, only 25,5% of passengers survived.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the proportion of the passengers that survived\n",
    "def plot_percentage_survivors(pclass, survivors_flag, axes):\n",
    "    class_all_passengers = len(titanic_data.query(\"pclass == @pclass\"))\n",
    "    class_survivors = len(titanic_data.query(\"pclass == @pclass & survived==@survivors_flag\"))\n",
    "    colors = ['blue', 'red']\n",
    "    explode = (0, 0.1)\n",
    "    labels = [\"Survived\", \"Died\"]\n",
    "    title='% of survived and died - Class {}'.format(pclass)\n",
    "    plot_data_pie([class_survivors, class_all_passengers-class_survivors], axes, labels, explode, title=title)\n",
    "\n",
    "#Plot data\n",
    "figure, axes = plt.subplots(1, 3, figsize=(16,5))\n",
    "\n",
    "plot_percentage_survivors(1, 1, axes[0]) # first class\n",
    "plot_percentage_survivors(2, 1, axes[1]) # second class\n",
    "plot_percentage_survivors(3, 1, axes[2]) # third class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 5. Calculate the proportion of the passengers that survived by *travel class* and *sex*. Present your results in *a single histogram*.</h4>\n",
    "<p>\n",
    "The function <i>survived_class_sex()</i> uses pandas.query() to get what we want to extract. <br>\n",
    "We plot an histogram that shows the survival proportion by travel class and by sex. The number are relevant : women survived more than men, in every class.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to get the proportion of passengers who survived by travel class and sex\n",
    "def survived_class_sex(pclass, sex, survived_flag):\n",
    "    survivors = len(titanic_data.query(\"pclass == @pclass & survived==@survived_flag & sex==@sex\"))\n",
    "    all_passengers_by_sex = len(titanic_data.query(\"pclass == @pclass & sex==@sex\"))\n",
    "    return survivors/all_passengers_by_sex\n",
    "\n",
    "male= []\n",
    "female= []\n",
    "\n",
    "for i in range(1,4):\n",
    "    male.append(survived_class_sex(i, 'male', 1))\n",
    "    female.append(survived_class_sex(i, 'female', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {'pclass': ['First class', 'Second class', 'Third class'],\n",
    "        'female': female,\n",
    "        'male': male }\n",
    "df = pd.DataFrame(raw_data, columns = ['pclass', 'female', 'male'])\n",
    "\n",
    "pos = list(range(len(df['pclass']))) \n",
    "width = 0.25 \n",
    "\n",
    "#Plotting the results\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "\n",
    "# Female bar\n",
    "plt.bar(pos, df.female, width, alpha=0.5, color='red', label=df['pclass'][0]) \n",
    "\n",
    "#Male bar\n",
    "plt.bar([p + width for p in pos], df.male, width, alpha=0.5, color='blue', label=df['pclass'][1]) \n",
    "\n",
    "# Plot parameters\n",
    "ax.set_ylabel('Survivors')\n",
    "ax.set_title('Survivors by travel class and sex')\n",
    "ax.set_xticks([p + 1.5 * width for p in pos])\n",
    "ax.set_xticklabels(df['pclass'])\n",
    "plt.xlim(min(pos)-width, max(pos)+width*3)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(['Female', 'Male'], loc='upper right')\n",
    "plt.grid()\n",
    "\n",
    "#At the end, show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h4> 6. Create 2 equally populated age categories and calculate survival proportions by age category, travel class and sex. Present your results in a DataFrame with unique index.</h4>\n",
    "<br>\n",
    "<p>\n",
    "First of all, we can see that many passengers does not have age, so we remove them from the dataset. Then, we sort passengers by age.<br>\n",
    "We compute the median value to split the population, in order to have two equally populated age categories (even if some passengers have the median age). The variable <i>splitter</i> enables the split, according to the median. We also use pandas <i>groupby()</i> as it enables to iterate over the groups.<br><br>\n",
    "The final DataFrame shows that women and younger passengers had a higher chance to survive. Therefore, the higher class a passenger was, the higher chance to survive he/she had.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look on the data\n",
    "people_unknown_age = titanic_data.age.isnull().sum()\n",
    "people_known_age = len(titanic_data) - people_unknown_age\n",
    "print (\"People with unknown age: \",people_unknown_age)\n",
    "print (\"People with known age: \", people_known_age)\n",
    "\n",
    "# Remove NaN values\n",
    "age_categories_df = titanic_data.copy()\n",
    "age_categories_df = age_categories_df.loc[~age_categories_df.age.isnull()].copy()\n",
    "\n",
    "# sorted by age, rows with age=NaN removed\n",
    "age_categories_df = age_categories_df.sort_values(by='age').copy()\n",
    "age_categories_df = age_categories_df.reset_index(drop=True)\n",
    "print (\"Shape of sorted by age without NaN, reindexed: \", age_categories_df.shape)\n",
    "\n",
    "# Compute median and max age\n",
    "median_age=age_categories_df.age.median()\n",
    "max_age=age_categories_df.age.max()\n",
    "\n",
    "# Get youngest passengers\n",
    "people_first_group_count = age_categories_df.query(\"age < @median_age\")\n",
    "print (\"First group count \", len(people_first_group_count))\n",
    "\n",
    "# Get oldest passengers\n",
    "people_second_group_count = age_categories_df.query(\"age > @median_age\")\n",
    "print (\"Second group count \", len(people_second_group_count))\n",
    "\n",
    "# Get number of passengers with median age\n",
    "people_with_median_age = age_categories_df.query(\"age == @median_age\")\n",
    "print (\"Number of people \", median_age, \" years old is \", len(people_with_median_age))\n",
    "\n",
    "\n",
    "# split into two groups, thanks to the splitter variable\n",
    "splitter = len(age_categories_df)//2\n",
    "print (\"Splitter: \", splitter) \n",
    "category_col = pd.Series()\n",
    "cate_1 = '(0.0-{}]'.format(median_age)\n",
    "cate_2 = '({}-{}]'.format(median_age, max_age)\n",
    "category_col  = [cate_1]*splitter + [cate_2]*splitter\n",
    "age_categories_df['age'] = category_col\n",
    "\n",
    "#Show that we have two equal groups\n",
    "print(age_categories_df.age.value_counts())\n",
    "\n",
    "# Group passengers by age, sex, and travel class\n",
    "age_categories_df.survived = age_categories_df.survived.astype('int') #To compute the mean just after\n",
    "data = age_categories_df.groupby(['age', 'pclass', 'sex'])['survived'].mean() * 100    #Get the percentage value\n",
    "data = pd.DataFrame(data)\n",
    "data.columns = ['% survived'] #Update column name for a better reading\n",
    "\n",
    "data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
